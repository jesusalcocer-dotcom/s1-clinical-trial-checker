# Automating S-1 Clinical Trial Disclosure Review: Design Narrative

## Why This Problem

When I worked as a capital markets associate, one of the most labor-intensive tasks was reviewing S-1 registration statements for biotech IPOs. Every drug candidate, every clinical trial mention, every characterization of safety or efficacy had to be checked against the actual trial record. The standard is strict: Securities Act Section 11 imposes near-strict liability on everyone who signs the filing if it contains a material misstatement or omission — and unlike fraud claims, the plaintiff does not need to prove the company intended to deceive. A single misleading sentence about a drug's tolerability profile can become the centerpiece of a class action.

The problem is that there is no single rulebook. The disclosure standards live in hundreds of individual SEC comment letters sent to companies over the years, a handful of enforcement actions, and a few Supreme Court decisions. A lawyer doing this review has to carry all of those precedents in their head, compare the filing's language against each one, and cross-reference claims against the clinical trial data on ClinicalTrials.gov. It is slow, it depends heavily on the reviewer's experience, and things get missed.

This makes it an excellent candidate for automation through a system that combines AI and mechanical code. The inputs are publicly available and structured: S-1 filings on EDGAR, clinical trial records on ClinicalTrials.gov, and legal standards that can be codified into reference files. The core task is comparison — does what the filing says match what the public record shows? — which is exactly the kind of work where code handles extraction and matching while an LLM handles contextual judgment. And the output is high-stakes enough that auditability is not optional: every conclusion must show its work.

## The Legal Framework

The law operates in three layers. At the base, Securities Act Section 11 and Rule 408 require that registration statements contain no material misstatements or omissions — including any information "necessary to make the required statements not misleading." Above that, Supreme Court decisions set the standards for how those rules apply. *Omnicare v. Laborers* (2015) established that even opinion statements — "we believe the drug is well-tolerated" — can be actionable if they omit known contrary facts or embed false factual claims. *Matrixx v. Siracusano* (2011) blocked the defense that adverse events are immaterial because they lack statistical significance. *TSC Industries v. Northway* (1976) defined materiality through the "total mix" standard — what a reasonable investor would want to know. At the enforcement level, SEC actions against companies like Clovis Oncology (inflated efficacy metrics), AVEO Pharmaceuticals (selective FDA disclosure), and the criminal conviction in *United States v. Harkonen* (leading with post-hoc subgroup results while burying a failed primary endpoint) provide concrete examples of what the SEC actually prosecutes.

## How the Skill Is Assembled

The tool is built as a Claude Code Skill — a structured prompt that orchestrates Python scripts and LLM analysis in a defined sequence. A Skill is superior to a normal prompt for this use case because it enforces a repeatable, step-by-step process rather than relying on the LLM to decide what to do next. The orchestrator (`SKILL.md`) defines the exact sequence of operations, the gates where the user must confirm before proceeding, and the rules for what code handles versus what the LLM assesses.

Four Python scripts in `scripts/` handle mechanical work: `edgar_fetch.py` downloads the S-1 from SEC EDGAR, `s1_parser.py` extracts drug candidates and clinical trial references from the HTML, `ctgov_fetch.py` retrieves structured trial data from the ClinicalTrials.gov API, and `comparison_builder.py` normalizes and matches elements between the two sources. Fourteen reference files in `reference/` contain the legal standards: operationalized check definitions with regex patterns and thresholds, verbatim SEC comment letter excerpts, the legal framework (statutes, cases, enforcement actions), guardrails for Layer 2 escalation, and a three-tier classification of red-flag phrases.

The key architectural choice is the separation between Layer 1 (eleven concrete checks that always run) and Layer 2 (legal escalation tests that fire only when Layer 1 flags issues). Layer 1 Pass 1 checks the S-1 against SEC standards using only the filing itself — phase labels, preclinical framing, comparative claims, FDA communication balance, red-flag phrases. Pass 2 brings in ClinicalTrials.gov data to compare trial design, endpoint hierarchy, safety characterizations, and data maturity against the public record. Layer 2 then applies the Supreme Court tests — Omnicare, Matrixx, Rule 408 pattern analysis — to any findings that survived Layer 1.

## Mechanisms for Accuracy and Auditability

Several design mechanisms distinguish this from a generic prompt. First, the LLM never receives open-ended questions. Every LLM call uses a slot-based prompt template where the exact S-1 text, the exact SEC comment letter language, and comparison pairs from real cases are pre-loaded by code. The LLM compares specific text against specific precedent — it does not research or reason from scratch. Second, the tool explicitly labels what was done by code (extraction, pattern matching, threshold detection) versus what was done by the LLM (contextual judgment, precedent comparison). An attorney reviewing the output can see each link in the reasoning chain. Third, red-flag phrases are classified into three tiers with explicit escalation rules: Tier 1 phrases like "safe" or "effective" are always flagged unless they appear in a Risk Factors section; Tier 2 phrases like "well-tolerated" are flagged only when unsupported by nearby quantitative data; Tier 3 phrases require contextual LLM assessment. This prevents both over-flagging and under-flagging. Fourth, calibrated language guardrails ensure the tool says "raises questions under Rule 408" rather than "violates Rule 408" — it identifies risk areas for attorney review, not legal conclusions.

## How We Iterated

The first version had the right checks but did not fully deliver on auditability. Findings appeared as conclusions — a status and a one-line summary — without showing the underlying comparison. The attorney had no way to trace how the tool reached "SIGNIFICANT RISK" versus "LOW RISK." The first major revision forced the full audit trail into the chat: for every check, the output now shows the extracted S-1 text, the legal standard being applied, and the LLM's reasoning. We also required the tool to narrate the legal journey — telling the user which part of the framework they are in and why — so someone without securities law experience could follow the analysis.

The second major revision overhauled the Layer 2 escalation tests. Previously, each test was roughly twenty lines: "apply the three-part test, show the result." We replaced these with detailed decision trees — the Omnicare test expanded from twenty lines to approximately one hundred and eighty, with six explicit nodes that the model must walk through in sequence. Each node asks a specific question ("Does this opinion embed a verifiable factual claim? Is that claim supported or contradicted by the ClinicalTrials.gov data?") and documents the answer before proceeding. The Rule 408 pattern test added section-placement analysis (are positive claims in the Prospectus Summary while caveats are buried in Risk Factors?) and enforcement pattern matching against Clovis, Harkonen, and AVEO. The Matrixx defense-blocker added the *Rigel* limitation — a critical check the original version missed entirely — which prevents over-protection by distinguishing findings where partial disclosure is consistent with top-line data from findings where the omission makes the stated claim misleading.

## Validation and Limitations

We tested the tool against a subset of five companies drawn from the SEC comment letters in our reference files — companies we knew had triggered SEC scrutiny. The tool correctly identified the disclosure patterns that the SEC had challenged in each case. Given more time, we would extend testing to synthetic data to stress-test edge cases: filings with unusual structures, drugs with complex multi-arm trial designs, and borderline characterizations where the tier classification is ambiguous. The current reference files cover twenty-three comment letter excerpts across seven topics; expanding this corpus would improve the comparison-pair mechanism and reduce the LLM's reliance on general reasoning in edge cases.

The tool does not replace attorney judgment. It systematizes the first pass — the mechanical, precedent-matching work that takes hours when done manually — and surfaces the findings that require human analysis. The design goal throughout was not to automate the legal conclusion but to automate the audit trail: show the text, show the law, show the comparison, and let the attorney decide.
